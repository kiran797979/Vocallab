# ğŸ§ª VocalLab â€” AI-Powered Chemistry Lab Instructor

<p align="center">
  <img src="https://raw.githubusercontent.com/kiran797979/Vocallab/master/assets/icon.png" width="120" height="120" alt="VocalLab Logo" />
</p>

<p align="center">
  <strong>Real-time AI Object Detection Â· Multilingual Voice Guidance Â· Live Instructor Dashboard</strong>
</p>

<p align="center">
  <a href="https://github.com/kiran797979/Vocallab"><img src="https://img.shields.io/badge/Status-Production_Ready-00d4aa?style=for-the-badge" /></a>
  <a href="https://expo.dev"><img src="https://img.shields.io/badge/Expo-SDK_52-61dafb?style=for-the-badge&logo=expo&logoColor=white" /></a>
  <a href="https://python.org"><img src="https://img.shields.io/badge/Python-3.10+-3776ab?style=for-the-badge&logo=python&logoColor=white" /></a>
  <a href="https://fastapi.tiangolo.com"><img src="https://img.shields.io/badge/FastAPI-0.115+-009688?style=for-the-badge&logo=fastapi&logoColor=white" /></a>
  <a href="https://ultralytics.com"><img src="https://img.shields.io/badge/Vision-YOLOv8-ED1C24?style=for-the-badge" /></a>
  <a href="https://www.amd.com/en/technologies/ryzen-ai"><img src="https://img.shields.io/badge/Accelerated_By-AMD_Ryzenâ„¢_AI-ED1C24?style=for-the-badge" /></a>
</p>

---

## ğŸ“– Table of Contents

1. [Vision & Mission](#-vision--mission)
2. [The Problem We Solve](#-the-problem-we-solve)
3. [How It Works (System Flow)](#-how-it-works-system-flow)
4. [Core Features](#-core-features)
5. [Technical Stack](#-technical-stack)
6. [Deep Dive: Computer Vision](#-deep-dive-computer-vision)
7. [Finite State Machine (FSM)](#-finite-state-machine-fsm)
8. [Multi-Language Voice Engine](#-multi-language-voice-engine)
9. [Safety & Compliance Engine](#-safety--compliance-engine)
10. [Setup & Installation](#-setup--installation)
11. [One-Command Launch](#-one-command-launch)
12. [Project Structure](#-project-structure)
13. [API & WebSocket Reference](#-api--websocket-reference)
14. [Configuration Guide](#-configuration-guide)
15. [Performance Benchmarks](#-performance-benchmarks)
16. [Troubleshooting](#-troubleshooting)
17. [Roadmap](#-roadmap)
18. [Contributing](#-contributing)
19. [License](#-license)

---

## ğŸš€ Vision & Mission

**VocalLab** is an initiative to democratize science education in India and beyond. Our vision is that no student's scientific curiosity should be limited by a lack of laboratory infrastructure.

By merging affordable smartphone technology with cutting-edge AI, VocalLab transforms any classroom, home, or rural education center into a fully guided chemistry laboratory â€” for zero hardware cost.

Our AI instructor is:
- **Accessible** â€” Works on any smartphone running Expo Go.
- **Multilingual** â€” Guides students in Hindi, English, Telugu, and Tamil.
- **Safe** â€” Monitors every student's setup simultaneously, flagging hazards in real time.
- **Scalable** â€” A single backend server can support an entire classroom of students.
- **Data-Driven** â€” Gives instructors live insights on all student progress from a premium dashboard.

---

## ğŸ§© The Problem We Solve

In India, over **500 million students** are enrolled in schools where:
1. **No lab equipment exists** â€” Physical glassware, chemicals, and instruments are unaffordable.
2. **Safety prevents practicals** â€” Schools skip experiments due to liability and chemical risks.
3. **Ratio is impossible** â€” One teacher cannot safely monitor 40 students simultaneously.
4. **Language is a barrier** â€” Quality lab software is exclusively in English.

**VocalLab solves all four problems** using AI proxies, real-time vision monitoring, multilingual voice, and a live instructor dashboard.

---

## âš™ï¸ How It Works (System Flow)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       STUDENT PHONE (Expo Go)                      â”‚
â”‚   Camera â†’ Base64 Frame â†’ WebSocket Send (2 FPS) â†’ [Backend]      â”‚
â”‚   [Backend] â†’ WebSocket Receive â†’ Detections + Guidance + Audio   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚ WebSocket ws://IP:8000/ws/student
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FASTAPI BACKEND (Python)                        â”‚
â”‚                                                                    â”‚
â”‚  1. Decode Base64 frame â†’ NumPy â†’ OpenCV                          â”‚
â”‚  2. YOLOv8 Inference â†’ List of {label, confidence, bbox}          â”‚
â”‚  3. Label Map â†’ COCO labels â†’ Lab equipment labels                â”‚
â”‚  4. FSM.process_detections() â†’ step_info, safety_alert, audio    â”‚
â”‚  5. Broadcast to Student WS + Dashboard WS                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ WebSocket /ws/student                 â”‚ WebSocket /ws/dashboard
            â–¼                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STUDENT GETS:      â”‚               â”‚  INSTRUCTOR GETS:          â”‚
â”‚  - Step guidance    â”‚               â”‚  - Live detection feed     â”‚
â”‚  - Bounding boxes   â”‚               â”‚  - All student progress    â”‚
â”‚  - Safety alerts    â”‚               â”‚  - Safety event log        â”‚
â”‚  - Voice audio      â”‚               â”‚  - Experiment stats        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ¨ Core Features

### ğŸ“± Mobile App (Student Interface)
- **Real-time Camera HUD** â€” Color-coded bounding boxes for every detected lab object.
- **Step Progress Panel** â€” Sliding panel shows current step, hint, progress bar, and missing items.
- **Missing Equipment Alerts** â€” Rolls up any items not yet in frame as pill badges.
- **Safety Flash Banner** â€” Full-width red alert with haptic vibration for proximity warnings.
- **Language Switcher** â€” Tap to cycle Hindi / English / Telugu / Tamil in real time.
- **Experiment Completion Screen** â€” Trophy animation on successful lab completion.
- **Auto-Reconnect** â€” WebSocket automatically reconnects if network drops, without losing state.
- **Smart IP Config** â€” Reads backend URL from `app.json` extra config, falls back to manual input.

### ğŸ“Š Instructor Dashboard (Real-time Monitor)
- **Glassmorphic Design** â€” Premium dark glass UI with animated mesh background and glowing accents.
- **Live Connection Indicator** â€” Animated pulsing dot shows backend WebSocket status.
- **Step Progress Panel** â€” Circular progress indicator + per-step status bars with animations.
- **Safety Status Panel** â€” Live feed of all safety alerts with severity badges (HIGH / CRITICAL).
- **Detection Feed** â€” Live list of all objects currently detected in the student's camera.
- **Event Log** â€” Timestamped chronological log of every step advance, detection, and alert.
- **Class Overview Tab** â€” Individual progress cards for each student in the class.
- **Experiment Library Tab** â€” Browse and activate experiments from the catalog.
- **Auto-Reconnect** â€” Dashboard silently reconnects to backend on disconnect.

### ğŸ§  Backend AI Engine
- **YOLOv8 Inference** â€” Sub-50ms detection on standard hardware; faster on AMD Ryzen AI.
- **FSM Step Logic** â€” Structured finite state machine manages experiment progression.
- **Safety Rule Engine** â€” Detects dangerous object proximity pairs and raises alerts.
- **Multilingual Audio Server** â€” Serves pre-generated MP3 audio files per language.
- **Multi-client WebSocket** â€” Handles many students simultaneously, each with isolated state.
- **Heartbeat Loop** â€” Async task sends heartbeat to dashboards every 25 seconds.
- **Full REST API** â€” Endpoints for health checks, stats, reset, and experiment info.

---

## ğŸ› ï¸ Technical Stack

| Layer | Technology | Version | Purpose |
|-------|-----------|---------|---------|
| **Mobile** | React Native (Expo) | SDK 52 | Student camera + guidance UI |
| **Mobile Camera** | expo-camera | ^16.x | Frame capture at 2 FPS |
| **Mobile Audio** | expo-av | ^15.x | Plays multilingual instruction audio |
| **Mobile Haptics** | expo-haptics | ^14.x | Safety vibration feedback |
| **Dashboard** | React + Vite | React 19 | Instructor real-time monitor |
| **Dashboard Styling** | Tailwind CSS v4 | ^4.x | Utility-first design system |
| **Backend Framework** | FastAPI | ^0.115 | REST API + async WebSocket server |
| **AI Model** | YOLOv8n (Ultralytics) | ^8.x | Object detection engine |
| **Image Processing** | OpenCV + NumPy | Latest | Frame decode + pre-processing |
| **ML Runtime** | PyTorch | ^2.6 | YOLO model inference backend |
| **WS Transport** | WebSockets (built-in) | â€” | Bidirectional real-time comms |
| **Audio Generation** | gTTS | ^2.x | Text-to-speech audio clip creation |
| **Acceleration** | AMD Ryzenâ„¢ AI NPU | â€” | Offload inference from CPU |

---

## ğŸ”¬ Deep Dive: Computer Vision

### YOLOv8 Detection Pipeline

```
Camera Frame (JPEG, quality=30%)
         â”‚
         â–¼
 base64 decode â†’ NumPy array
         â”‚
         â–¼
 cv2.imdecode() â†’ BGR frame
         â”‚
         â–¼
 YOLO.predict(frame, conf=0.30, imgsz=640)
         â”‚
         â–¼
 for each box:
   â”œâ”€ Extract: x1, y1, x2, y2, confidence, class_id
   â”œâ”€ Map class_id â†’ YOLO name (e.g. "bottle")
   â”œâ”€ Map YOLO name â†’ Lab label (e.g. "beaker")
   â””â”€ Emit: {label, confidence, bbox, center}
         â”‚
         â–¼
 Return: List[Detection], frame_width, frame_height
```

### Proxy Label Mapping System

VocalLab's most innovative feature: **everyday household items as lab equipment proxies**. This lets students practice the exact workflow of a chemistry lab without owning the glassware.

| YOLO COCO Label | VocalLab Lab Label | Use Case |
|---|---|---|
| `bottle` | `beaker` | Main reaction vessel |
| `wine glass` | `conical_flask` | Titration flask |
| `cup` | `measuring_cylinder` | Volume measurement |
| `mouse` | `dropper` | Reagent addition |
| `book` | `lab_manual` | Recording observations |
| `cell phone` | `ph_meter` | pH measurement |
| `remote` | `thermometer` | Temperature monitoring |
| `keyboard` | `hotplate` | Heating equipment |
| `bowl` | `petri_dish` | Sample collection |
| `spoon` | `spatula` | Solid handling |
| `knife` | `glass_rod` | Stirring |
| `scissors` | `tongs` | Heat-resistant handling |
| `vase` | `volumetric_flask` | Precise volume |
| `laptop` | `analytical_balance` | Mass measurement |
| `clock` | `stopwatch` | Reaction timing |
| `banana` | `test_tube` | Small sample reactions |

### AMD Ryzen AI Acceleration

When running on a machine with an AMD Ryzen AI processor (Ryzen 7000-series and above), the NPU (Neural Processing Unit) takes over inference tasks from the CPU, delivering:
- **~50% lower CPU usage** during inference
- **Cooler thermals** for sustained lab sessions
- **Consistent latency** of 15-25ms regardless of other workloads

---

## ğŸ”„ Finite State Machine (FSM)

The FSM is the brain of the experiment logic. It is a structured state machine that knows **exactly** what state the experiment is in and what must happen to advance.

### State Machine Logic

```python
class ExperimentFSM:
    # State: which step index are we on?
    current_step_index = 0
    stable_count = 0                    # Frames where all objects were detected
    stable_frames_required = 3          # 3 consecutive frames to advance

    def process_detections(self, detections):
        required = current_step.required_objects
        detected = {d.label for d in detections}

        if not required.issubset(detected):
            stable_count = 0            # Reset if anything is missing
            # Auto-play step intro if first time on this step
            return result

        stable_count += 1
        if stable_count >= stable_frames_required:
            # ADVANCE TO NEXT STEP
            current_step_index += 1
            stable_count = 0
            result["step_advance"] = True
```

### Experiment Configuration (`config/experiment.json`)

```json
{
  "name": "Acid-Base Titration",
  "total_steps": 4,
  "steps": [
    {
      "id": 0,
      "name": "Setup Equipment",
      "required_objects": ["beaker", "conical_flask"],
      "hint_en": "Place the beaker and conical flask on the table",
      "hint_hi": "à¤¬à¥€à¤•à¤° à¤”à¤° à¤•à¥‹à¤¨à¤¿à¤•à¤² à¤«à¥à¤²à¤¾à¤¸à¥à¤• à¤Ÿà¥‡à¤¬à¤² à¤ªà¤° à¤°à¤–à¥‡à¤‚",
      "hint_te": "à°¬à±€à°•à°°à± à°®à°°à°¿à°¯à± à°•à±‹à°¨à°¿à°•à°²à± à°«à±à°²à°¾à°¸à±à°•à± à°Ÿà±‡à°¬à±à°²à± à°®à±€à°¦ à°‰à°‚à°šà°‚à°¡à°¿",
      "hint_ta": "à®ªà¯€à®•à¯à®•à®°à¯ à®®à®±à¯à®±à¯à®®à¯ à®•à¯‚à®®à¯à®ªà¯ à®•à¯à®Ÿà¯à®µà¯ˆà®¯à¯ˆ à®®à¯‡à®šà¯ˆà®¯à®¿à®²à¯ à®µà¯ˆà®¯à¯à®™à¯à®•à®³à¯",
      "audio_intro": "step_1_intro",
      "audio_complete": "step_1_complete"
    }
  ],
  "safety_rules": {
    "proximity_threshold": 150,
    "alert_cooldown_seconds": 3,
    "dangerous_pairs": [
      ["hand", "beaker"],
      ["hand", "conical_flask"],
      ["hand", "measuring_cylinder"]
    ]
  }
}
```

### How to Add a New Experiment

1. Create a new JSON file in `backend/config/`.
2. Follow the schema above â€” define steps, hints for 4 languages, and safety rules.
3. Update `ExperimentFSM(config_path="config/my_new_experiment.json")` in `main.py`.
4. Run `python generate_audio.py` to generate audio assets.
5. Restart the backend.

---

## ğŸ—£ï¸ Multi-Language Voice Engine

VocalLab provides **real-time multilingual audio guidance** in 4 Indian languages.

### Audio Asset Pipeline

```
generate_audio.py
       â”‚
       â–¼
  Google TTS (gTTS)
       â”‚
       â–¼  generates
 backend/audio/
   â”œâ”€â”€ en/
   â”‚    â”œâ”€â”€ step_1_intro.mp3
   â”‚    â”œâ”€â”€ step_1_complete.mp3
   â”‚    â””â”€â”€ error_hand_proximity.mp3
   â”œâ”€â”€ hi/   (Hindi)
   â”œâ”€â”€ te/   (Telugu)
   â””â”€â”€ ta/   (Tamil)
       â”‚
       â–¼
FastAPI StaticFiles mounts /audio/* at startup
       â”‚
       â–¼
Backend sends: { audio_url: "/audio/hi/step_1_intro.mp3" }
       â”‚
       â–¼
Mobile fetches full URL: http://10.x.x.x:8000/audio/hi/step_1_intro.mp3
       â”‚
       â–¼
expo-av plays audio on device speaker
```

### Language Switch Flow

When a student taps the language button:
1. Mobile sends `{ type: "language_change", language: "te" }` over WebSocket.
2. Backend resets the intro audio tracker for the current step.
3. Backend sends back `{ type: "language_updated", step_info: {...}, audio_url: "..." }`.
4. Mobile updates the hint text and immediately plays the step's intro in Telugu.

### Audio Priority System

Audio playback has two modes:
- **Normal** (`force=false`): Only plays if no other audio is playing.
- **Forced** (`force=true`): Interrupts any playing audio immediately. Used for:
  - Safety alerts (always critical)
  - Step advances (always immediate)
  - Language changes (always replaces old language)

---

## ğŸ›¡ï¸ Safety & Compliance Engine

Safety is the highest priority in VocalLab. The FSM contains a dedicated safety check that runs on **every single frame** before any step logic.

### Dangerous Proximity Detection

```python
def _check_safety(self, detections):
    for pair in self.dangerous_pairs:
        obj_a = find_detection(pair[0])
        obj_b = find_detection(pair[1])
        if obj_a and obj_b:
            dist = euclidean_distance(obj_a.center, obj_b.center)
            if dist < self.proximity_threshold:
                # TRIGGER SAFETY ALERT
                return {
                    "severity": "high",
                    "message": f"Keep hands away from {pair[1]}!",
                    "distance_px": dist
                }
```

### Safety Response Chain

```
Dangerous pair detected (e.g. hand + beaker)
         â”‚
         â”œâ”€â”€â–¶ Backend: sets safety_alert in response
         â”‚
         â”œâ”€â”€â–¶ Mobile: red full-width banner appears on camera view
         â”‚
         â”œâ”€â”€â–¶ Mobile: expo-haptics fires NotificationFeedbackType.Error
         â”‚
         â”œâ”€â”€â–¶ Mobile: plays /audio/{lang}/error_hand_proximity.mp3 (force=true)
         â”‚
         â”œâ”€â”€â–¶ Dashboard: safety panel shows alert card with severity badge
         â”‚
         â””â”€â”€â–¶ Event Log: timestamped entry "âš  Keep hands away from beaker!"
```

### Safety Configuration

| Parameter | Default | Description |
|---|---|---|
| `proximity_threshold` | `150px` | Pixel distance between object centers to trigger alert |
| `alert_cooldown_seconds` | `3s` | Minimum time between repeated alerts for the same pair |
| `dangerous_pairs` | `[hand+beaker, hand+flask, ...]` | List of object pairs that must never be too close |

---

## ğŸš€ Setup & Installation

### Prerequisites

| Requirement | Version | Purpose |
|---|---|---|
| Node.js | v18+ | Mobile + Dashboard builds |
| Python | 3.10+ | Backend AI engine |
| pip | Latest | Python package manager |
| Expo Go App | Latest | Run mobile app on your phone |
| Git | Any | Version control |

### Backend Setup

```powershell
# 1. Navigate to backend
cd backend

# 2. Create virtual environment
python -m venv ..\venv

# 3. Activate virtual environment (Windows)
..\venv\Scripts\activate

# 4. Install all Python dependencies
pip install fastapi uvicorn ultralytics opencv-python-headless numpy gtts python-multipart websockets

# 5. Generate multilingual audio assets
python generate_audio.py

# 6. Start the server
python main.py
# Server is live at http://localhost:8000
```

### Mobile Setup

```powershell
# 1. Navigate to mobile
cd mobile

# 2. Install dependencies
npm install --legacy-peer-deps

# 3. Update your PC's IP address in app.json
# Edit: "backendUrl": "http://YOUR_PC_IP:8000"

# 4. Start Expo (LAN mode â€” most stable)
npx expo start --lan -c

# 5. Scan QR code with Expo Go on your phone
# Ensure phone and PC are on the same Wi-Fi
```

### Dashboard Setup

```powershell
# 1. Navigate to dashboard
cd dashboard

# 2. Install dependencies
npm install

# 3. Start development server
npm run dev
# Dashboard opens at http://localhost:5173
```

---

## âš¡ One-Command Launch

A master PowerShell script is included to launch **all three services simultaneously** in separate terminal windows.

```powershell
# From the project root:
.\launch-vocallab.ps1
```

This will:
1. Open a **Backend window** â€” activates `venv` and starts `uvicorn`.
2. Open a **Dashboard window** â€” starts the Vite dev server.
3. Open a **Mobile window** â€” starts Expo in LAN mode.

> **Note**: Run this script from the **project root directory** (`c:\...\vocallab`), not from any subfolder.

---

## ğŸ“ Project Structure

```
vocallab/
â”‚
â”œâ”€â”€ launch-vocallab.ps1         # Master launcher for all services
â”‚
â”œâ”€â”€ backend/                    # Python AI Engine
â”‚   â”œâ”€â”€ main.py                 # FastAPI server, WebSocket routes, startup
â”‚   â”œâ”€â”€ generate_audio.py       # TTS script to pre-generate all audio files
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â”‚
â”‚   â”œâ”€â”€ engine/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ detector.py         # YOLOv8 wrapper â€” detect_frame(), detect_base64()
â”‚   â”‚   â””â”€â”€ fsm.py              # Experiment Finite State Machine
â”‚   â”‚
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ experiment.json     # Acid-Base Titration config (steps, hints, safety)
â”‚   â”‚   â””â”€â”€ label_map.py        # COCO label â†’ Lab equipment proxy mapping
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ yolov8_titration.pt # (Optional) Custom-trained YOLO model
â”‚   â”‚
â”‚   â””â”€â”€ audio/
â”‚       â”œâ”€â”€ en/                 # English .mp3 instruction files
â”‚       â”œâ”€â”€ hi/                 # Hindi .mp3 instruction files
â”‚       â”œâ”€â”€ te/                 # Telugu .mp3 instruction files
â”‚       â””â”€â”€ ta/                 # Tamil .mp3 instruction files
â”‚
â”œâ”€â”€ mobile/                     # React Native Student App
â”‚   â”œâ”€â”€ App.js                  # Main app â€” Home, Connection, Experiment screens
â”‚   â”œâ”€â”€ app.json                # Expo config â€” permissions, backendUrl, wsUrl
â”‚   â”œâ”€â”€ package.json            # JS dependencies (Expo SDK 52)
â”‚   â””â”€â”€ fix-and-run.ps1         # Cache-clear + restart automation script
â”‚
â”œâ”€â”€ dashboard/                  # React Instructor Dashboard
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx             # Main dashboard component â€” all 3 tabs
â”‚   â”‚   â””â”€â”€ index.css           # Global styles â€” glassmorphism, animations
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ vite.config.js          # Vite build config with React plugin
â”‚   â”œâ”€â”€ tailwind.config.js      # Tailwind v4 tokens
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ training/                   # Model fine-tuning scripts (YOLOv8 custom)
â”œâ”€â”€ assets/                     # App icon and splash screen images
â””â”€â”€ README.md                   # This file
```

---

## ğŸ”Œ API & WebSocket Reference

### REST Endpoints

| Endpoint | Method | Auth | Description |
|----------|--------|------|-------------|
| `/` | `GET` | None | Server info + uptime + model status |
| `/health` | `GET` | None | Health check â€” model loaded, FSM state |
| `/experiment` | `GET` | None | Full FSM state as JSON |
| `/experiment/steps` | `GET` | None | All step definitions |
| `/detect` | `POST` | None | Single-frame detection (base64 image) |
| `/reset` | `POST` | None | Reset FSM + notify all connected students |
| `/stats` | `GET` | None | Frame count, detections, uptime, connections |
| `/docs` | `GET` | None | FastAPI auto-generated Swagger UI |

#### Example: `/health` Response

```json
{
  "status": "healthy",
  "model_loaded": true,
  "fsm_loaded": true,
  "fsm_state": {
    "experiment_name": "Acid-Base Titration",
    "total_steps": 4,
    "current_step": 1,
    "completed": false
  },
  "dashboard_clients": 1,
  "student_clients": 3
}
```

### WebSocket: Student (`ws://IP:8000/ws/student`)

#### Client â†’ Server Messages

```json
// Send a camera frame for detection
{ "type": "frame", "data": "<base64_jpeg>", "language": "hi", "timestamp": 1708876543 }

// Change guidance language
{ "type": "language_change", "language": "te" }

// Ping for latency measurement
{ "type": "ping" }
```

#### Server â†’ Client Messages

```json
// Welcome on connect â€” full experiment state
{
  "type": "welcome",
  "experiment_name": "Acid-Base Titration",
  "total_steps": 4,
  "current_step": 0,
  "step_info": { ... },
  "model_loaded": true
}

// Detection result â€” sent after every frame
{
  "type": "detection_result",
  "detections": [
    { "label": "beaker", "confidence": 0.87, "bbox": [120, 60, 340, 280], "center": [230, 170] }
  ],
  "step_info": {
    "current_step": 0,
    "total_steps": 4,
    "step_name": "Setup Equipment",
    "hint": "à¤¬à¥€à¤•à¤° à¤”à¤° à¤•à¥‹à¤¨à¤¿à¤•à¤² à¤«à¥à¤²à¤¾à¤¸à¥à¤• à¤Ÿà¥‡à¤¬à¤² à¤ªà¤° à¤°à¤–à¥‡à¤‚",
    "required_objects": ["beaker", "conical_flask"],
    "missing_objects": ["conical_flask"],
    "progress": 50.0,
    "step_status": "active"
  },
  "safety_alert": null,
  "audio_url": "/audio/hi/step_1_intro.mp3",
  "step_advance": false,
  "experiment_complete": false
}

// Safety violation
{
  "type": "detection_result",
  "safety_alert": {
    "severity": "high",
    "message": "Keep hands away from beaker!",
    "pair": ["hand", "beaker"],
    "distance_px": 87.3
  },
  "audio_url": "/audio/hi/error_hand_proximity.mp3"
}

// Language change confirmed
{ "type": "language_updated", "language": "te", "step_info": { ... }, "audio_url": "/audio/te/step_1_intro.mp3" }

// Pong response
{ "type": "pong", "timestamp": "2026-02-25T09:00:00Z" }
```

### WebSocket: Dashboard (`ws://IP:8000/ws/dashboard`)

#### Server â†’ Dashboard Messages

```json
// Initial state on connect
{ "type": "experiment_loaded", "experiment_name": "...", "total_steps": 4, "current_step": 0 }

// Forwarded student data on every frame
{
  "type": "student_update",
  "detections": [ ... ],
  "step_info": { ... },
  "safety_alert": null,
  "step_advance": false,
  "experiment_complete": false,
  "student_stats": { "frames": 142, "detections": 407, "alerts": 0 }
}

// Student lifecycle events
{ "type": "student_connected", "student_count": 3 }
{ "type": "student_disconnected", "student_count": 2 }

// Experiment reset
{ "type": "experiment_reset", "current_step": 0 }

// Backend heartbeat (every 25s)
{ "type": "heartbeat", "timestamp": "2026-02-25T09:00:25Z", "server_version": "2.0.0" }
```

---

## âš™ï¸ Configuration Guide

### `mobile/app.json` â€” Backend Connection

```json
{
  "expo": {
    "extra": {
      "backendUrl": "http://192.168.1.100:8000",
      "wsUrl": "ws://192.168.1.100:8000/ws/student"
    }
  }
}
```

> **Important**: Replace `192.168.1.100` with your PC's actual local IP address.  
> Find it with `ipconfig` on Windows or `ifconfig` on Mac/Linux.

### `backend/config/experiment.json` â€” Add a Step

```json
{
  "id": 4,
  "name": "Measure pH",
  "required_objects": ["ph_meter", "conical_flask"],
  "hint_en": "Dip the pH meter into the flask and record the reading",
  "hint_hi": "pH à¤®à¥€à¤Ÿà¤° à¤•à¥‹ à¤«à¥à¤²à¤¾à¤¸à¥à¤• à¤®à¥‡à¤‚ à¤¡à¥à¤¬à¥‹à¤à¤‚ à¤”à¤° à¤°à¥€à¤¡à¤¿à¤‚à¤— à¤¨à¥‹à¤Ÿ à¤•à¤°à¥‡à¤‚",
  "hint_te": "pH à°®à±€à°Ÿà°°à±â€Œà°¨à± à°«à±à°²à°¾à°¸à±à°•à±â€Œà°²à±‹ à°®à±à°‚à°šà°¿ à°°à±€à°¡à°¿à°‚à°—à± à°¨à±‹à°Ÿà± à°šà±‡à°¯à°‚à°¡à°¿",
  "hint_ta": "pH à®®à¯€à®Ÿà¯à®Ÿà®°à¯ˆ à®•à¯à®Ÿà¯à®µà¯ˆà®¯à®¿à®²à¯ à®¨à®©à¯ˆà®¤à¯à®¤à¯ à®…à®³à®µà¯€à®Ÿà¯à®Ÿà¯ˆ à®ªà®¤à®¿à®µà¯ à®šà¯†à®¯à¯à®¯à¯à®™à¯à®•à®³à¯",
  "audio_intro": "step_5_intro",
  "audio_complete": "step_5_complete"
}
```

### `backend/config/label_map.py` â€” Add a Proxy

```python
YOLO_TO_LAB = {
    # ... existing mappings ...
    "umbrella": "burette",      # Add new proxy here
}
```

---

## ğŸ“Š Performance Benchmarks

| Metric | Standard Laptop CPU | GPU (CUDA) | AMD Ryzenâ„¢ AI NPU |
|--------|--------------------|-----------|--------------------|
| Inference Latency | ~85ms | ~12ms | **~18ms** |
| Backend FPS (full) | 1.2 FPS | 5.0 FPS | **2.5 FPS** |
| CPU Usage | 95% | 20% | **15%** |
| Power Draw | High | Medium | **Ultra Low** |
| Thermal Throttle | Frequent | Rare | **None** |

> **Recommended**: AMD Ryzen 7000-series (or newer) with NPU support for best classroom deployment.

### Frame Rate Strategy

VocalLab intentionally caps the AI inference rate at **~2 FPS** (700ms capture loop). This is by design:
- Chemistry steps take **seconds to minutes**, not milliseconds.
- Running at 2 FPS preserves battery and thermal performance.
- The camera HUD renders at the device's **native refresh rate** (60 FPS) for smooth visual experience.
- The capture loop has a **processing lock** (`isProcessingFrame`) to prevent frame queue buildup.

---

## ğŸ› ï¸ Troubleshooting

### Mobile Issues

| Symptom | Cause | Fix |
|---------|-------|-----|
| `"Internet connection appears offline"` | Phone can't reach PC on local network | Switch to LAN mode; ensure both on same Wi-Fi |
| `"Metro Bundler cache error"` | Stale cache | Run `npx expo start -c` to clear cache |
| `"Cannot read undefined"` error | Old JS bundle cached | Clear Expo Go app cache on phone, restart |
| App crashes on launch | React Native incompatible style props | Ensure no web-only CSS (`backdropFilter`, `shadowBlur`) in styles |
| QR code not scanning | Screen brightness | Increase screen brightness; use direct scan |

### Backend Issues

| Symptom | Cause | Fix |
|---------|-------|-----|
| `"Detector FAILED"` on startup | YOLO model not downloaded | Wait for auto-download, or manually place `yolov8n.pt` in `backend/` |
| `"FSM FAILED"` on startup | Invalid `experiment.json` | Validate JSON syntax; check all required fields exist |
| `"Address already in use"` | Port 8000 occupied | Run `taskkill /F /IM python.exe` (Windows) or `pkill -f uvicorn` |
| No audio playing | Audio directory missing | Run `python generate_audio.py` in `backend/` |
| CORS error in browser | Browser security | Dashboard is pre-configured; use `localhost` URL |

### Network Issues

| Symptom | Cause | Fix |
|---------|-------|-----|
| `"Connection refused"` | Backend not running | Start backend first with `python main.py` |
| `"Ngrok tunnel failed"` | Ngrok service outage | Use `--lan` mode instead of `--tunnel` |
| Dashboard shows "OFFLINE" | WS connection blocked | Check Windows Firewall; allow port 8000 |
| Audio plays in wrong language | App cache | Restart the experiment on the phone |

---

## ğŸ—ºï¸ Roadmap

### Phase 2 (Next 3 Months)
- [ ] **Custom YOLO Training** â€” Collect real lab glassware dataset for 95%+ accuracy.
- [ ] **Student Accounts** â€” Login system with experiment history persistence.
- [ ] **More Experiments** â€” Newton's Laws (Physics), Cell Observation (Biology), Electrolysis.
- [ ] **Teacher Controls** â€” Pause/resume experiment for individual students from dashboard.

### Phase 3 (6 Months)
- [ ] **AR Integration** â€” Augmented Reality overlays showing virtual liquid levels and color indicators.
- [ ] **Collaborative Mode** â€” Two students share one experiment state from separate phones.
- [ ] **Cloud Sync** â€” Firebase/Supabase backend for cloud experiment history.
- [ ] **Offline Mode** â€” Download experiment pack for areas with no internet.

### Phase 4 (12 Months)
- [ ] **Government Curriculum Integration** â€” Align experiment steps with NCERT Class 10/12 syllabus.
- [ ] **Competitive Mode** â€” Leaderboard for fastest accurate experiment completion.
- [ ] **Regional Language Expansion** â€” Kannada, Marathi, Bengali, Gujarati.
- [ ] **VocalLab Cloud** â€” Hosted SaaS for schools without their own infrastructure.

---

## ğŸ¤ Contributing

We welcome contributions from educators, developers, and researchers!

```bash
# 1. Fork the repository
# 2. Clone your fork
git clone https://github.com/YOUR_USERNAME/Vocallab.git

# 3. Create a feature branch
git checkout -b feature/MyAwesomeFeature

# 4. Make your changes and commit
git add .
git commit -m "feat: add MyAwesomeFeature"

# 5. Push and open a Pull Request
git push origin feature/MyAwesomeFeature
```

### Good First Issues
- Adding a new experiment JSON configuration.
- Adding a new proxy label mapping.
- Translating hints to a new regional language.
- Improving dashboard mobile responsiveness.

---

## âš–ï¸ License

Distributed under the **MIT License**.

```
MIT License

Copyright (c) 2026 VocalLab Team

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software...
```

See `LICENSE` for full text.

---

## ğŸ“¬ Contact & Support

**VocalLab Team**  
GitHub: [@kiran797979](https://github.com/kiran797979)  
Repository: [https://github.com/kiran797979/Vocallab](https://github.com/kiran797979/Vocallab)

---

<p align="center">
  <strong>VocalLab â€” Transforming every smartphone into a chemistry laboratory.</strong><br/>
  <em>Built with â¤ï¸ for 500 million students who deserve better science education.</em>
</p>

---

*Last updated: February 25, 2026 | VocalLab v2.0.0 | Expo SDK 52 | FastAPI 0.115*
