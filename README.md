# ğŸ§ª VocalLab â€” AI-Powered Chemistry Lab Instructor

[![Status](https://img.shields.io/badge/Status-Production-00d4aa?style=for-the-badge)](https://github.com/kiran797979/Vocallab)
[![Python](https://img.shields.io/badge/Python-3.10+-3776ab?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![React Native](https://img.shields.io/badge/React%20Native-Expo-61dafb?style=for-the-badge&logo=react&logoColor=black)](https://expo.dev)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)

> **Real-time AI lab assistant that guides students through chemistry experiments using computer vision, voice instructions, and safety monitoring â€” in 4 Indian languages.**

---

## ğŸ¯ What It Does

- ğŸ“± **Phone becomes a lab assistant** â€” point camera at equipment, get step-by-step voice guidance
- ğŸ”¬ **YOLOv8 object detection** â€” identifies beakers, flasks, droppers, and 20+ lab items in real-time
- ğŸ—£ï¸ **4-language support** â€” Hindi (à¤¹à¤¿à¤‚à¤¦à¥€), English, Telugu (à°¤à±†à°²à±à°—à±), Tamil (à®¤à®®à®¿à®´à¯) voice instructions
- ğŸ›¡ï¸ **Safety monitoring** â€” alerts when hands get too close to chemicals with haptic feedback
- ğŸ“Š **Instructor dashboard** â€” real-time monitoring of student progress, detections, and safety events

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    WebSocket     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    WebSocket    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“± Mobile  â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚  ğŸ–¥ï¸  Backend     â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚ ğŸ“Š Dashboardâ”‚
â”‚  (Expo RN)  â”‚   frames/results â”‚  (FastAPI+YOLO)  â”‚   live updates â”‚  (React+Vite)â”‚
â”‚             â”‚                  â”‚                  â”‚                â”‚             â”‚
â”‚ â€¢ Camera    â”‚                  â”‚ â€¢ YOLOv8 detect  â”‚                â”‚ â€¢ Step view â”‚
â”‚ â€¢ Audio     â”‚                  â”‚ â€¢ FSM engine     â”‚                â”‚ â€¢ Safety logâ”‚
â”‚ â€¢ Haptics   â”‚                  â”‚ â€¢ Audio serve    â”‚                â”‚ â€¢ Detection â”‚
â”‚ â€¢ Bbox draw â”‚                  â”‚ â€¢ Safety check   â”‚                â”‚ â€¢ Events    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                 AMD Ryzenâ„¢ AI
```

---

## ğŸ› ï¸ Tech Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Backend** | FastAPI + Python 3.10+ | REST/WebSocket server, orchestration |
| **Detection** | YOLOv8 (Ultralytics) | Real-time object detection |
| **Mobile** | React Native (Expo) | Camera, audio, haptic feedback |
| **Dashboard** | React + Vite + Tailwind v4 | Instructor monitoring interface |
| **Voice** | Google TTS (gTTS) | Multi-language audio generation |
| **Hardware** | AMD Ryzenâ„¢ AI | Accelerated AI inference |

---

## âœ¨ Features

### ğŸ“± Mobile App
- Real-time camera with bounding box overlay
- Step-by-step guidance with progress tracking
- Missing objects indicator ("ğŸ” Need: conical_flask, dropper")
- Language cycling (Hindi â†” English â†” Telugu â†” Tamil)
- Safety alert banner with haptic feedback
- Audio playback for instructions and warnings
- Auto-reconnecting WebSocket

### ğŸ“Š Instructor Dashboard
- Live experiment progress with step timeline
- Detected objects panel with confidence scores
- Safety alerts log with severity levels
- Real-time event log
- Class overview with mock student cards
- Experiment library browser

### ğŸ–¥ï¸ Backend
- ConnectionManager for multiple students + dashboards
- Frame rate limiting (2 FPS max)
- Per-student stats tracking
- Server stats endpoint
- Startup validation of audio files
- PyTorch 2.6 compatibility patch
- Heartbeat to keep dashboards alive

---

## ğŸ”¬ Experiment: Acid-Base Titration

| Step | Name | Required Objects | Audio |
|------|------|-----------------|-------|
| 1 | Setup Equipment | `beaker`, `conical_flask` | step_1_intro.mp3 |
| 2 | Pour Acid (HCl) | `beaker`, `conical_flask`, `dropper` | step_2_intro.mp3 |
| 3 | Add Base & Indicator | `beaker`, `conical_flask`, `dropper` | step_3_intro.mp3 |
| 4 | Record Observations | `beaker`, `lab_manual` | step_4_intro.mp3 |

**Real props mapping:** bottle â†’ beaker, wine glass â†’ conical_flask, mouse â†’ dropper, book â†’ lab_manual, person â†’ hand

---

## ğŸ·ï¸ Label Mapping (YOLO COCO â†’ Lab Equipment)

| YOLO Class | Lab Equipment | YOLO Class | Lab Equipment |
|-----------|---------------|-----------|---------------|
| bottle | beaker | person | hand |
| wine glass | conical_flask | cell phone | ph_meter |
| cup | measuring_cylinder | remote | thermometer |
| bowl | petri_dish | mouse | dropper |
| spoon | spatula | keyboard | hotplate |
| knife | glass_rod | laptop | analytical_balance |
| scissors | tongs | book | lab_manual |
| vase | volumetric_flask | clock | stopwatch |
| banana | test_tube | apple | rubber_stopper |
| orange | watch_glass | carrot | stirring_rod |
| toothbrush | brush | pen | pipette |

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10+
- Node.js 18+
- Expo CLI (`npx expo`)
- Phone with Expo Go app

### 1ï¸âƒ£ Backend
```bash
cd backend

# Create virtual environment
python -m venv ../venv
# Windows:
..\venv\Scripts\activate
# macOS/Linux:
# source ../venv/bin/activate

# Install dependencies
pip install fastapi uvicorn websockets ultralytics opencv-python-headless numpy Pillow gtts aiofiles python-multipart torch torchvision

# Generate audio files (first time only)
python generate_audio.py

# Run server
python -m uvicorn main:app --host 0.0.0.0 --port 8000
```

### 2ï¸âƒ£ Dashboard
```bash
cd dashboard
npm install
npm run dev
# â†’ http://localhost:3000
```

### 3ï¸âƒ£ Mobile
```bash
cd mobile
npm install
npx expo start
# Scan QR code with Expo Go on your phone
```

---

## ğŸ“ Project Structure

```
vocallab/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ experiment.json      # Experiment steps, safety rules, hints
â”‚   â”‚   â””â”€â”€ label_map.py         # YOLO â†’ lab equipment mapping
â”‚   â”œâ”€â”€ engine/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ detector.py          # YOLOv8 wrapper with label mapping
â”‚   â”‚   â””â”€â”€ fsm.py               # Experiment state machine
â”‚   â”œâ”€â”€ audio/
â”‚   â”‚   â”œâ”€â”€ en/ (16 mp3)         # English audio
â”‚   â”‚   â”œâ”€â”€ hi/ (16 mp3)         # Hindi audio
â”‚   â”‚   â”œâ”€â”€ te/ (16 mp3)         # Telugu audio
â”‚   â”‚   â””â”€â”€ ta/ (16 mp3)         # Tamil audio
â”‚   â”œâ”€â”€ main.py                  # FastAPI server (REST + WebSocket)
â”‚   â”œâ”€â”€ generate_audio.py        # Generate TTS audio files
â”‚   â”œâ”€â”€ test_model.py            # Model testing utility
â”‚   â””â”€â”€ yolov8n.pt               # YOLOv8 nano model
â”œâ”€â”€ mobile/
â”‚   â”œâ”€â”€ App.js                   # Expo React Native app
â”‚   â”œâ”€â”€ app.json                 # Expo configuration
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx              # React dashboard
â”‚   â”‚   â”œâ”€â”€ index.css            # Tailwind + custom styles
â”‚   â”‚   â””â”€â”€ main.jsx             # Entry point
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ vite.config.js
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ”Œ API Endpoints

| Method | Endpoint | Description |
|--------|---------|-------------|
| `GET` | `/` | Server status + uptime |
| `GET` | `/health` | Detailed health check |
| `GET` | `/experiment` | Current experiment state |
| `GET` | `/experiment/steps` | Step definitions |
| `POST` | `/detect` | Run YOLO on base64 image |
| `POST` | `/reset` | Reset experiment FSM |
| `GET` | `/stats` | Server statistics |
| `WS` | `/ws/student` | Mobile app WebSocket |
| `WS` | `/ws/dashboard` | Dashboard WebSocket |

---

## ğŸ¬ Demo Script (2 minutes)

1. **Open** dashboard on laptop â†’ show "Waiting for student..."
2. **Open** mobile app â†’ connect to server â†’ select Hindi
3. **Start** experiment â†’ camera view with "Step 1: Setup Equipment"
4. **Point** camera at bottle (â†’beaker) and wine glass (â†’flask) â†’ boxes appear
5. **Watch** progress bar fill â†’ step advances to Step 2
6. **Bring** hand close to beaker â†’ ğŸš¨ SAFETY ALERT with haptic
7. **Show** dashboard â†’ live step updates, detection list, safety log
8. **Switch** language to Telugu â†’ hint text changes immediately
9. **Complete** remaining steps â†’ ğŸ‰ experiment complete overlay

---

## ğŸŒ Impact

> **Built for 500M+ Indian students** who lack access to quality laboratory experiences. VocalLab transforms any smartphone into an AI-powered lab instructor, bridging the gap between theoretical knowledge and practical skills.

- ğŸ« **Rural schools** â€” no expensive lab equipment tracking needed
- ğŸ—£ï¸ **Regional languages** â€” instructions in students' native tongue
- ğŸ›¡ï¸ **Lab safety** â€” AI monitoring prevents accidents
- ğŸ“Š **Teacher insight** â€” real-time dashboard for remote supervision
- ğŸ’° **Zero cost** â€” uses everyday objects as lab equipment proxies

---

## âš¡ Powered by AMD Ryzenâ„¢ AI

VocalLab leverages AMD Ryzenâ„¢ AI for accelerated YOLOv8 inference, enabling real-time object detection at 2+ FPS with minimal latency on consumer hardware.

---

<p align="center">
  <b>ğŸ§ª VocalLab</b> â€” Making science education accessible, safe, and multilingual.
  <br/>
  <sub>Built with â¤ï¸ for the AMD Pervasive AI Developer Contest</sub>
</p>
